{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112e1321",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "112e1321",
        "outputId": "4e167635-9836-44ef-f552-de7062bb3732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "\n",
        "# If running on Colab, uncomment the next line.\n",
        "%pip -q install spacy rapidfuzz sentence-transformers transformers accelerate faiss-cpu PyMuPDF pandas openpyxl tqdm word2number\n",
        "\n",
        "import sys, os, re, json, math, string, itertools, pathlib, textwrap, typing\n",
        "from typing import List, Dict, Tuple, Optional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84c7baea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84c7baea",
        "outputId": "42e3c3ec-b8b4-4fd6-a86f-e8293fe6bbda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ON_COLAB: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, asdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import spacy\n",
        "from rapidfuzz import fuzz\n",
        "import numpy as np\n",
        "\n",
        "import fitz  \n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util as sbert_util\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "try:\n",
        "    from google.colab import files\n",
        "    ON_COLAB = True\n",
        "except Exception:\n",
        "    ON_COLAB = False\n",
        "\n",
        "print(\"ON_COLAB:\", ON_COLAB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24fa4d2",
      "metadata": {
        "id": "f24fa4d2"
      },
      "outputs": [],
      "source": [
        "\n",
        "ATTRIBUTE_EXCEL_PATH = 'data/Attribute Dictionary.xlsx'   \n",
        "TEMPLATE_FILES = ['data/TN_Standard_Template_Redacted_extracted_text.txt','data/WA_Standard_Template_Redacted_extracted_text.txt']         \n",
        "CONTRACT_FILES = ['data/TN_Contract1_Redacted.pdf','data/WA_2_Redacted.pdf']         \n",
        "\n",
        "ATTR_COL_CANDIDATES = ['Attribute']\n",
        "\n",
        "KEYWORDS_COL_CANDIDATES = ['Keywords']\n",
        "\n",
        "REGEX_COL_CANDIDATES = ['Regex']\n",
        "\n",
        "MAX_TARGET_ATTRIBUTES = 5\n",
        "TARGET_ATTRIBUTES = [\n",
        "    \"Claims Submission & Adjudication\",\n",
        "    \"Compensation / Fee Schedule\",\n",
        "    \"Termination\",\n",
        "    \"Use of Symbols and Marks\",\n",
        "    \"Confidentiality / Provider Information\"\n",
        "]\n",
        "\n",
        "EXCEPTION_TOKENS = [\n",
        "    'except', 'unless', 'provided that',\n",
        "    'subject to', 'however,', 'save that',\n",
        "    'notwithstanding', 'only if'\n",
        "]\n",
        "\n",
        "PLACEHOLDER_MAP = {\n",
        "    # Percentages (XX%, 100%, 95% etc.)\n",
        "    r\"\\[\\(?\\s*XX\\s*%\\s*\\)?\\]\": \"<PCT>\",                # generic percentage placeholder\n",
        "    r\"\\b\\d{1,3}\\s*%\\b\": \"<PCT>\",                       # numeric percentages like 100%, 95%\n",
        "    r\"\\b(one\\s*hundred|ninety[-\\s]*five|fifty)\\s*percent\\b\": \"<PCT>\",\n",
        "\n",
        "    # Compensation / Fee references\n",
        "    r\"\\b(Fee\\s+Schedule|Compensation\\s+Schedule|Plan\\s+Compensation\\s+Schedule|WCS|PCS)\\b\": \"<FEE_SCHEDULE>\",\n",
        "    r\"\\b(Rate|Eligible\\s+Charge[s]?)\\b\": \"<RATE>\",\n",
        "\n",
        "    # Parties / Organization\n",
        "    r\"\\b(Plan|Company|Network|Agency|Affiliate|Other\\s+Payors?)\\b\": \"<ORG>\",\n",
        "    r\"\\b(Provider|Participating\\s+Provider)\\b\": \"<PROVIDER>\",\n",
        "\n",
        "    # Members\n",
        "    r\"\\b(Member|Enrollee|Subscriber|Insured|Beneficiary|Covered\\s+(Person|Individual)|Dependent)\\b\": \"<MEMBER>\",\n",
        "\n",
        "    # Programs\n",
        "    r\"\\b(Government\\s+Program|Medicare|Medicaid|CMS|HCA)\\b\": \"<GOV_PROGRAM>\",\n",
        "\n",
        "    # Documents\n",
        "    r\"\\b(Participation\\s+Attachment[s]?)\\b\": \"<ATTACHMENT>\",\n",
        "    r\"\\b(provider\\s+manual\\(s\\))\\b\": \"<PROVIDER_MANUAL>\",\n",
        "    r\"\\b(Health\\s+Benefit\\s+Plan)\\b\": \"<PLAN_DOC>\",\n",
        "\n",
        "    # Payments\n",
        "    r\"\\b(Cost\\s*Share[s]?|copayment[s]?|coinsurance|deductible[s]?)\\b\": \"<COST_SHARE>\",\n",
        "    r\"\\b(Claim[s]?)\\b\": \"<CLAIM>\",\n",
        "\n",
        "    # Legal placeholders\n",
        "    r\"\\b(Regulatory\\s+Requirements?)\\b\": \"<REG_REQ>\",\n",
        "    r\"\\b(Effective\\s+Date|MM/DD/YYYY)\\b\": \"<DATE>\",\n",
        "    r\"\\[\\s*_{2,}\\s*\\]\": \"<BLANK>\",   # underscores for blanks like [_________]\n",
        "\n",
        "    # Misc\n",
        "    r\"\\b(Health\\s+Services?|Covered\\s+Services?)\\b\": \"<SERVICE>\",\n",
        "    r\"\\b(Medically\\s+Necessary|Medical\\s+Necessity)\\b\": \"<MEDICAL_NECESSITY>\",\n",
        "}\n",
        "\n",
        "\n",
        "FUZZY_THRESHOLD = 92             \n",
        "SBERT_THRESHOLD = 0.86            \n",
        "SBERT_AMBIG_LOW, SBERT_AMBIG_HIGH = 0.75, 0.86\n",
        "\n",
        "# Model toggles\n",
        "USE_SPACY_MODEL = \"en_core_web_sm\"     # or \"en_core_web_trf\" for larger (slower) transformer pipeline\n",
        "USE_SBERT_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"   \n",
        "USE_DEBERTA_CROSS_ENCODER = False\n",
        "DEBERTA_CE_MODEL = \"cross-encoder/nli-deberta-v3-large\"     # optional heavy model for pair scoring\n",
        " \n",
        "OUT_DIR = Path(\"/notebooks/outputs\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc4f10e",
      "metadata": {
        "id": "7fc4f10e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "from word2number import w2n \n",
        "\n",
        "def normalize_whitespace(s: str) -> str:\n",
        "    s = re.sub(r'\\s+', ' ', s or '').strip()\n",
        "    return s\n",
        "\n",
        "def to_ascii_lower(s: str) -> str:\n",
        "    return normalize_whitespace(s).lower()\n",
        "\n",
        "def apply_placeholders(s: str) -> str:\n",
        "    \"\"\"Replace known placeholders to canonical tokens for fair comparison.\"\"\"\n",
        "    out = s\n",
        "\n",
        "    # 1. Replace known placeholders from config\n",
        "    for pat, repl in PLACEHOLDER_MAP.items():\n",
        "        out = re.sub(pat, repl, out, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2. Normalize percentages written as digits (e.g., \"95 percent\" → \"95%\")\n",
        "    out = re.sub(r'(\\d+)\\s*percent', r'\\1%', out, flags=re.IGNORECASE)\n",
        "\n",
        "    # 3. Normalize percentages written in words (e.g., \"ninety five percent\" → \"95%\")\n",
        "    def word_percent_to_num(match):\n",
        "        words = match.group(1).lower()\n",
        "        try:\n",
        "            num = w2n.word_to_num(words)   \n",
        "            return f\"{num}%\"\n",
        "        except ValueError:\n",
        "            return match.group(0)  \n",
        "\n",
        "    out = re.sub(r'\\b([a-z\\s-]+)\\s+percent\\b', word_percent_to_num, out, flags=re.IGNORECASE)\n",
        "\n",
        "    return out\n",
        "\n",
        "def normalize_for_compare(s: str) -> str:\n",
        "    s = apply_placeholders(s)\n",
        "    # drop punctuation but keep %\n",
        "    keep_percent = '%'\n",
        "    punct = ''.join(ch for ch in r\"\"\"!\"#$&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\"\" if ch != '%')\n",
        "    s = s.translate(str.maketrans('', '', punct))\n",
        "    s = to_ascii_lower(s)\n",
        "    return s\n",
        "\n",
        "def contains_exception_tokens(text: str, template_has_exception: bool = False) -> bool:\n",
        "    text_l = to_ascii_lower(text)\n",
        "    if template_has_exception:\n",
        "        return False\n",
        "    return any(tok in text_l for tok in EXCEPTION_TOKENS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0152523",
      "metadata": {
        "id": "c0152523"
      },
      "outputs": [],
      "source": [
        "\n",
        "def autodetect_column(df: pd.DataFrame, candidates: list) -> Optional[str]:\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    for cand in candidates:\n",
        "        if cand.lower() in cols:\n",
        "            return cols[cand.lower()]\n",
        "    # fallback: fuzzy find by substring\n",
        "    for c in df.columns:\n",
        "        cl = c.lower()\n",
        "        if any(k.lower() in cl for k in candidates):\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class AttributeSpec:\n",
        "    name: str\n",
        "    keywords: List[str]\n",
        "    regexes: List[str]\n",
        "\n",
        "def load_attribute_dictionary(xlsx_path: str) -> List[AttributeSpec]:\n",
        "    df = pd.read_excel(xlsx_path, sheet_name=0)\n",
        "    attr_col = autodetect_column(df, ATTR_COL_CANDIDATES)\n",
        "    if not attr_col:\n",
        "        raise ValueError(f\"Could not find attribute column. Checked: {ATTR_COL_CANDIDATES}. Found columns: {df.columns.tolist()}\")\n",
        "\n",
        "    kw_col = autodetect_column(df, KEYWORDS_COL_CANDIDATES)\n",
        "    rx_col = autodetect_column(df, REGEX_COL_CANDIDATES)\n",
        "\n",
        "    specs = []\n",
        "    for _, row in df.iterrows():\n",
        "        name = str(row[attr_col]).strip()\n",
        "        if not name or name.lower() in ['nan', 'none']:\n",
        "            continue\n",
        "        keywords = []\n",
        "        regexes = []\n",
        "        if kw_col and not pd.isna(row.get(kw_col, None)):\n",
        "            keywords = [normalize_whitespace(x) for x in re.split(r'[;,]', str(row[kw_col])) if str(x).strip()]\n",
        "        if rx_col and not pd.isna(row.get(rx_col, None)):\n",
        "            regexes = [normalize_whitespace(x) for x in re.split(r'[;,]', str(row[rx_col])) if str(x).strip()]\n",
        "\n",
        "        specs.append(AttributeSpec(name=name, keywords=keywords, regexes=regexes))\n",
        "\n",
        "    seen = set()\n",
        "    uniq_specs = []\n",
        "    for spec in specs:\n",
        "        if spec.name not in seen:\n",
        "            uniq_specs.append(spec)\n",
        "            seen.add(spec.name)\n",
        "        if len(uniq_specs) >= MAX_TARGET_ATTRIBUTES:\n",
        "            break\n",
        "\n",
        "    print(\"Loaded attributes:\", [s.name for s in uniq_specs])\n",
        "    return uniq_specs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deeaf6ab",
      "metadata": {
        "id": "deeaf6ab"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TemplateClause:\n",
        "    name: str                 # e.g., 'TN' or 'WA'\n",
        "    raw_text: str\n",
        "    norm_text: str\n",
        "    has_exception_tokens: bool\n",
        "\n",
        "def read_text_file(p: Path) -> str:\n",
        "    return p.read_text(encoding='utf-8', errors='ignore')\n",
        "\n",
        "def load_templates(paths: List[str]) -> List[TemplateClause]:\n",
        "    tpls = []\n",
        "    for path in paths:\n",
        "        p = Path(path)\n",
        "        if not p.exists():\n",
        "            print(f\"[WARN] Template not found: {p}\")\n",
        "            continue\n",
        "        raw = read_text_file(p)\n",
        "        has_exc = contains_exception_tokens(raw, template_has_exception=False)\n",
        "        tpls.append(TemplateClause(\n",
        "            name=p.stem,\n",
        "            raw_text=raw,\n",
        "            norm_text=normalize_for_compare(raw),\n",
        "            has_exception_tokens=has_exc\n",
        "        ))\n",
        "    if not tpls:\n",
        "        raise ValueError(\"No templates loaded. Please upload 1–2 template .txt files.\")\n",
        "    print(\"Templates:\", [t.name for t in tpls])\n",
        "    return tpls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b509c4",
      "metadata": {
        "id": "41b509c4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def pdf_to_text(path: str) -> str:\n",
        "    doc = fitz.open(path)\n",
        "    texts = []\n",
        "    for page in doc:\n",
        "        texts.append(page.get_text(\"text\"))\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "def is_pdf(path: str) -> bool:\n",
        "    return Path(path).suffix.lower() == '.pdf'\n",
        "\n",
        "def split_into_clauses(text: str) -> List[Dict]:\n",
        "    paras = [normalize_whitespace(p) for p in re.split(r'\\n\\s*\\n+', text) if normalize_whitespace(p)]\n",
        "    clauses = []\n",
        "    clause_id = 1\n",
        "    for para in paras:\n",
        "        splits = re.split(r'(?<=[.;])\\s+(?=[A-Z(\\d])|(?<=: )\\s+', para)\n",
        "        for sp in splits:\n",
        "            s = normalize_whitespace(sp)\n",
        "            if len(s) < 5:\n",
        "                continue\n",
        "            clauses.append({\n",
        "                \"clause_id\": clause_id,\n",
        "                \"text\": s,\n",
        "                \"norm\": normalize_whitespace(s.lower())     \n",
        "            })\n",
        "            clause_id += 1\n",
        "    return clauses\n",
        "\n",
        "path = 'data/TN_Contract1_Redacted.pdf'\n",
        "text = pdf_to_text(path)\n",
        "clauses = split_into_clauses(text)\n",
        "print(len(clauses))\n",
        "for c in clauses[:10]:\n",
        "    print(c)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930347db",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class AttributeSpec:\n",
        "    def __init__(self, name: str, keywords: List[str], regexes: List[str]):\n",
        "        self.name = name\n",
        "        self.keywords = [k.lower().strip() for k in keywords if k.strip()]\n",
        "        self.regexes = [r.strip() for r in regexes if r.strip()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"AttributeSpec(name={self.name})\"\n",
        "\n",
        "ATTRIBUTE_SEEDS = {\n",
        "    \"Medicaid Timely Filing\": {\n",
        "        \"keywords\": [\"medicaid\", \"timely filing\", \"claim submission\", \"days\"],\n",
        "        \"regexes\":  [r\"\\bmedicaid\\b.*\\b\\d{1,3}\\s*days?\\b\"]\n",
        "    },\n",
        "    \"Medicare Timely Filing\": {\n",
        "        \"keywords\": [\"medicare\", \"timely filing\", \"claim submission\", \"days\"],\n",
        "        \"regexes\":  [r\"\\bmedicare\\b.*\\b\\d{1,3}\\s*days?\\b\"]\n",
        "    },\n",
        "    \"No Steerage/SOC\": {\n",
        "        \"keywords\": [\"steerage\", \"soc\", \"freedom of choice\"],\n",
        "        \"regexes\":  [r\"\\bno\\s+steerage\\b\", r\"\\bSOC\\b\"]\n",
        "    },\n",
        "    \"Medicaid Fee Schedule\": {\n",
        "        \"keywords\": [\"medicaid fee schedule\"],\n",
        "        \"regexes\":  [r\"\\bmedicaid\\b.*\\bfee schedule\\b\"]\n",
        "    },\n",
        "    \"Medicare Fee Schedule\": {\n",
        "        \"keywords\": [\"medicare fee schedule\"],\n",
        "        \"regexes\":  [r\"\\bmedicare\\b.*\\bfee schedule\\b\"]\n",
        "    },\n",
        "}\n",
        "\n",
        "def load_specs_from_excel(path: str) -> List[AttributeSpec]:\n",
        "    df = pd.read_excel(path)\n",
        "    specs = []\n",
        "    for _, row in df.iterrows():\n",
        "        name = str(row[\"Attribute\"]).strip()\n",
        "        if name in [\"nan\", \"\", \"None\"]:\n",
        "            continue\n",
        "        if name not in ATTRIBUTE_SEEDS:   \n",
        "            continue\n",
        "        kws = ATTRIBUTE_SEEDS[name][\"keywords\"]\n",
        "        rxs = ATTRIBUTE_SEEDS[name][\"regexes\"]\n",
        "        specs.append(AttributeSpec(name, kws, rxs))\n",
        "    return specs\n",
        "\n",
        "def split_into_clauses(text: str) -> List[Dict]:\n",
        "    paras = [normalize_whitespace(p) for p in re.split(r'\\n\\s*\\n+', text) if normalize_whitespace(p)]\n",
        "    clauses = []\n",
        "    clause_id = 1\n",
        "    for para in paras:\n",
        "        splits = re.split(r'(?<=[.;])\\s+(?=[A-Z(\\d])|(?<=: )\\s+', para)\n",
        "        for sp in splits:\n",
        "            s = normalize_whitespace(sp)\n",
        "            if len(s) < 5:\n",
        "                continue\n",
        "            clauses.append({\"clause_id\": clause_id, \"text\": s})\n",
        "            clause_id += 1\n",
        "    return clauses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DgFRg5p4q2WZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgFRg5p4q2WZ",
        "outputId": "79c6aa56-d0de-49c4-8dde-074013f8b286"
      },
      "outputs": [],
      "source": [
        "# ------------------ Only regex ------------------\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional\n",
        "import fitz \n",
        "\n",
        "\n",
        "def detect_attribute_for_clause_using_regex(clause_text: str, specs: List[AttributeSpec]) -> Optional[str]:\n",
        "    t = clause_text.lower()\n",
        "    for spec in specs:\n",
        "        if spec.name in [\"Medicaid Timely Filing\", \"Medicare Timely Filing\"]:\n",
        "            payer = spec.name.split()[0].lower()\n",
        "            if re.search(rf\"\\b{payer}\\b.*\\b\\d{{1,3}}\\s*days?\\b\", t) or \\\n",
        "               (\"timely filing\" in t and payer in t):\n",
        "                return spec.name\n",
        "            continue\n",
        "\n",
        "        elif spec.name in [\"Medicaid Fee Schedule\", \"Medicare Fee Schedule\"]:\n",
        "            payer = spec.name.split()[0].lower()\n",
        "            if payer in t and re.search(r\"\\bfee schedule\\b\", t):\n",
        "                return spec.name\n",
        "\n",
        "        elif spec.name == \"No Steerage/SOC\":\n",
        "            if re.search(r\"\\bsteerage\\b\", t) or re.search(r\"\\bsoc\\b\", t) or \"freedom of choice\" in t:\n",
        "                return spec.name\n",
        "\n",
        "        for rx in spec.regexes:\n",
        "            if re.search(rx, clause_text, flags=re.IGNORECASE):\n",
        "                return spec.name\n",
        "\n",
        "        if any(kw in t for kw in spec.keywords):\n",
        "            return spec.name\n",
        "\n",
        "    return None\n",
        "\n",
        "excel_path = \"data/Attribute Dictionary.xlsx\"\n",
        "pdf_path = \"data/TN_Contract1_Redacted.pdf\"\n",
        "\n",
        "specs = load_specs_from_excel(excel_path)\n",
        "text = pdf_to_text(pdf_path) \n",
        "clauses = split_into_clauses(text)\n",
        "\n",
        "for c in clauses:\n",
        "    attr = detect_attribute_for_clause_using_regex(c[\"text\"], specs)\n",
        "    if attr:\n",
        "        print(f\"ID {c['clause_id']} → Attribute: {attr}\\nText: {c['text']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nmj1xHHe6AUE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmj1xHHe6AUE",
        "outputId": "1b286bab-30b4-4562-a46a-6158f49d7356"
      },
      "outputs": [],
      "source": [
        "# ------------------ Using regix and spacy ------------------\n",
        "import re\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional\n",
        "import fitz  \n",
        "import spacy\n",
        "\n",
        "\n",
        "def init_spacy(model: str = \"en_core_web_sm\"):\n",
        "    try:\n",
        "        nlp = spacy.load(model)\n",
        "    except OSError:\n",
        "        raise OSError(f\"spaCy model '{model}' not found. Install via: python -m spacy download {model}\")\n",
        "    return nlp\n",
        "\n",
        "def detect_attribute_for_clause_spacy_regex(\n",
        "    clause_text: str, specs: List[AttributeSpec], nlp=None\n",
        ") -> Optional[str]:\n",
        "    \"\"\"Detect attributes using regex first, then spaCy lemma-based keywords with strict payer context.\"\"\"\n",
        "    if not clause_text or not clause_text.strip():\n",
        "        return None\n",
        "\n",
        "    text_lower = clause_text.lower()\n",
        "    doc = nlp(clause_text) if nlp else None\n",
        "\n",
        "    for spec in specs:\n",
        "        payer = spec.name.split()[0].lower() if \" \" in spec.name else \"\"\n",
        "\n",
        "        for rx in spec.regexes:\n",
        "            try:\n",
        "                if re.search(rx, clause_text, flags=re.IGNORECASE):\n",
        "                    return f\"{spec.name} (regex)\"\n",
        "            except re.error:\n",
        "                pass\n",
        "\n",
        "        if doc:\n",
        "            lemmas = {token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct}\n",
        "\n",
        "            if spec.name in [\"Medicaid Timely Filing\", \"Medicare Timely Filing\"]:\n",
        "                if \"timely\" in lemmas and \"filing\" in lemmas and payer in text_lower:\n",
        "                    return f\"{spec.name} (spacy)\"\n",
        "                if re.search(r\"\\b\\d{1,3}\\s*days?\\b\", text_lower) and payer in text_lower:\n",
        "                    return f\"{spec.name} (spacy)\"\n",
        "\n",
        "            elif spec.name in [\"Medicaid Fee Schedule\", \"Medicare Fee Schedule\"]:\n",
        "                if \"fee\" in lemmas and \"schedule\" in lemmas and payer in text_lower:\n",
        "                    return f\"{spec.name} (spacy)\"\n",
        "\n",
        "            elif spec.name == \"No Steerage/SOC\":\n",
        "                if \"steerage\" in lemmas or \"soc\" in lemmas or \"freedom of choice\" in text_lower:\n",
        "                    return f\"{spec.name} (spacy)\"\n",
        "\n",
        "    return None\n",
        "\n",
        "nlp = init_spacy()\n",
        "\n",
        "excel_path = \"data/Attribute Dictionary.xlsx\"\n",
        "pdf_path = \"data/TN_Contract1_Redacted.pdf\"\n",
        "\n",
        "specs = load_specs_from_excel(excel_path)\n",
        "text = pdf_to_text(pdf_path)\n",
        "clauses = split_into_clauses(text)\n",
        "\n",
        "for c in clauses:\n",
        "    attr = detect_attribute_for_clause_spacy_regex(c[\"text\"], specs, nlp)\n",
        "    if attr:\n",
        "        print(f\"ID {c['clause_id']} → Attribute: {attr}\\nText: {c['text']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b053b2e",
      "metadata": {
        "id": "6b053b2e"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SimilarityEngines:\n",
        "    def __init__(self, sbert_model_name: str, use_cross_encoder: bool, cross_encoder_name: str):\n",
        "        self.sbert = SentenceTransformer(sbert_model_name)\n",
        "        self.use_cross_encoder = use_cross_encoder\n",
        "        self.cross_encoder = None\n",
        "        self.ce_tokenizer = None\n",
        "        if use_cross_encoder:\n",
        "            self.cross_encoder = AutoModelForSequenceClassification.from_pretrained(cross_encoder_name)\n",
        "            self.ce_tokenizer = AutoTokenizer.from_pretrained(cross_encoder_name)\n",
        "            self.cross_encoder.eval()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sbert_score(self, a: str, b: str) -> float:\n",
        "        embs = self.sbert.encode([a, b], convert_to_tensor=True, normalize_embeddings=True)\n",
        "        sim = sbert_util.cos_sim(embs[0], embs[1]).item()\n",
        "        return float(sim)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def cross_encoder_score(self, a: str, b: str) -> Optional[float]:\n",
        "        if not self.use_cross_encoder or self.cross_encoder is None:\n",
        "            return None\n",
        "        inputs = self.ce_tokenizer(a, b, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        logits = self.cross_encoder(**inputs).logits\n",
        "        if logits.shape[-1] == 3:\n",
        "            probs = torch.softmax(logits, dim=-1).squeeze(0)\n",
        "            entail = probs[-1].item()   \n",
        "            return float(entail)\n",
        "        return torch.sigmoid(logits).mean().item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0a31a2",
      "metadata": {
        "id": "8f0a31a2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class StepResult:\n",
        "    step: str\n",
        "    satisfied: bool\n",
        "    score: Optional[float]\n",
        "    comment: str\n",
        "\n",
        "@dataclass\n",
        "class ClauseDecision:\n",
        "    clause_id: int\n",
        "    attribute: Optional[str]\n",
        "    template_used: Optional[str]\n",
        "    label: str                # \"Standard\" | \"Non-Standard\" | \"Ambiguous\" | \"Skip\"\n",
        "    score: float\n",
        "    rule: str\n",
        "    steps: List[StepResult]\n",
        "    text: str\n",
        "\n",
        "def classify_against_template(clause, template: TemplateClause, engines: SimilarityEngines) -> Tuple[str, float, str, List[StepResult]]:\n",
        "    steps = []\n",
        "    c_raw, c_norm = clause[\"text\"], clause[\"norm\"]\n",
        "    t_norm = template.norm_text\n",
        "\n",
        "    # Step A: Exception/condition tokens (Non-Standard if present in clause, absent in template)\n",
        "    has_exc = contains_exception_tokens(c_raw, template_has_exception=template.has_exception_tokens)\n",
        "    steps.append(StepResult(\"exception_check\", has_exc, None, \"Detected conditional/exception tokens in clause; template lacks them.\"))\n",
        "    if has_exc:\n",
        "        return \"Non-Standard\", 0.90, \"new_condition\", steps\n",
        "\n",
        "    # Step B: Exact normalized match\n",
        "    exact = (c_norm == t_norm)\n",
        "    steps.append(StepResult(\"exact_normalized_match\", exact, None, \"Clause equals template after normalization.\"))\n",
        "    if exact:\n",
        "        return \"Standard\", 0.99, \"exact_norm\", steps\n",
        "\n",
        "    # Step C: Placeholder-aware equality (no-op if same as normalized equality)\n",
        "    placeholder_like = (c_norm == t_norm)\n",
        "    steps.append(StepResult(\"placeholder_substitution\", placeholder_like, None, \"Placeholders/value substitutions align (e.g., percent).\"))\n",
        "    if placeholder_like:\n",
        "        return \"Standard\", 0.95, \"placeholder_subst\", steps\n",
        "\n",
        "    # Step D: Fuzzy lexical similarity\n",
        "    lex = fuzz.ratio(c_norm, t_norm)\n",
        "    steps.append(StepResult(\"fuzzy_lexical\", lex >= FUZZY_THRESHOLD, float(lex)/100.0, f\"RapidFuzz ratio={lex}\"))\n",
        "    if lex >= FUZZY_THRESHOLD:\n",
        "        return \"Standard\", 0.90, \"lexical_high\", steps\n",
        "\n",
        "    # Step E: Semantic similarity (SBERT)\n",
        "    sbert_sim = engines.sbert_score(c_raw, template.raw_text)\n",
        "    steps.append(StepResult(\"semantic_sbert\", sbert_sim >= SBERT_THRESHOLD, sbert_sim, f\"SBERT cosine={sbert_sim:.3f}\"))\n",
        "    if sbert_sim >= SBERT_THRESHOLD:\n",
        "        return \"Standard\", 0.85, \"semantic_high\", steps\n",
        "\n",
        "    if SBERT_AMBIG_LOW <= sbert_sim < SBERT_AMBIG_HIGH:\n",
        "        steps.append(StepResult(\"semantic_ambiguous_band\", True, sbert_sim, \"SBERT score in ambiguous range; needs review.\"))\n",
        "        return \"Ambiguous\", sbert_sim, \"semantic_ambiguous\", steps\n",
        "\n",
        "    # Step F: Optional DeBERTa v3 Large cross-encoder (if enabled)\n",
        "    if engines.use_cross_encoder:\n",
        "        ce = engines.cross_encoder_score(c_raw, template.raw_text)\n",
        "        steps.append(StepResult(\"deberta_cross_encoder\", ce is not None and ce >= 0.7, ce, \"Cross-encoder entailment prob (>=0.7 → Standard).\"))\n",
        "        if ce is not None and ce >= 0.7:\n",
        "            return \"Standard\", float(ce), \"deberta_ce_high\", steps\n",
        "\n",
        "    # Step G: Default Non-Standard\n",
        "    steps.append(StepResult(\"default_nonstandard\", True, sbert_sim, \"Low similarity and no earlier rule satisfied.\"))\n",
        "    return \"Non-Standard\", float(sbert_sim), \"low_similarity\", steps\n",
        "\n",
        "def choose_best_template(clause, templates: List[TemplateClause], engines: SimilarityEngines):\n",
        "    ranked = []\n",
        "    for tpl in templates:\n",
        "        label, score, rule, steps = classify_against_template(clause, tpl, engines)\n",
        "        ranked.append((tpl.name, label, score, rule, steps))\n",
        "\n",
        "    for tpl_name, label, score, rule, steps in ranked:\n",
        "        if label == \"Non-Standard\" and any(s.step == \"exception_check\" and s.satisfied for s in steps):\n",
        "            return tpl_name, label, score, rule, steps\n",
        "\n",
        "    def score_key(x):\n",
        "        tpl_name, label, score, rule, steps = x\n",
        "        priority = {\"Standard\": 3, \"Ambiguous\": 2, \"Non-Standard\": 1}.get(label, 0)\n",
        "        return (priority, score)\n",
        "\n",
        "    tpl_name, label, score, rule, steps = sorted(ranked, key=score_key, reverse=True)[0]\n",
        "    return tpl_name, label, score, rule, steps\n",
        "\n",
        "def classify_clauses(clauses: List[Dict], specs: List[AttributeSpec], templates: List[TemplateClause], engines: SimilarityEngines, nlp=None):\n",
        "    decisions = []\n",
        "    for cl in clauses:\n",
        "        # FIX: call the correct detector\n",
        "        attr = detect_attribute_for_clause_spacy_regex(cl[\"text\"], specs, nlp)\n",
        "\n",
        "        if not attr:\n",
        "            decisions.append(ClauseDecision(\n",
        "                clause_id=cl[\"clause_id\"], attribute=None, template_used=None,\n",
        "                label=\"Skip\", score=0.0, rule=\"no_target_attribute\", steps=[], text=cl[\"text\"]\n",
        "            ))\n",
        "            continue\n",
        "\n",
        "        tpl_name, label, score, rule, steps = choose_best_template(cl, templates, engines)\n",
        "        decisions.append(ClauseDecision(\n",
        "            clause_id=cl[\"clause_id\"], attribute=attr, template_used=tpl_name,\n",
        "            label=label, score=score, rule=rule, steps=steps, text=cl[\"text\"]\n",
        "        ))\n",
        "    return decisions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc291b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "ddc291b5",
        "outputId": "079c787a-ae50-44e0-b7e1-77270eb41a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded attributes: ['Medicaid Timely Filing', 'Medicare Timely Filing', 'No Steerage/SOC', 'Medicaid Fee Schedule', 'Medicare Fee Schedule']\n",
            "Templates: ['TN_Standard_Template_Redacted_extracted_text', 'WA_Standard_Template_Redacted_extracted_text']\n",
            "SBERT model loaded: sentence-transformers/all-MiniLM-L6-v2\n",
            "DeBERTa CE enabled: False\n",
            "Contract files: ['/content/hilabs/TN_Contract1_Redacted.pdf', '/content/hilabs/WA_2_Redacted.pdf']\n",
            "TN_Contract1_Redacted.pdf: Extracted 564 clauses\n",
            "WA_2_Redacted.pdf: Extracted 886 clauses\n",
            "Summary by attribute/label:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "Out of range float values are not JSON compliant: nan",
              "type": "dataframe",
              "variable_name": "summary"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4372f004-6e4e-4c5b-836e-3258da9940c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attribute</th>\n",
              "      <th>label</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4372f004-6e4e-4c5b-836e-3258da9940c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4372f004-6e4e-4c5b-836e-3258da9940c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4372f004-6e4e-4c5b-836e-3258da9940c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_e4096ec8-70d2-4866-ac8d-672459a3c0d2\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summary')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e4096ec8-70d2-4866-ac8d-672459a3c0d2 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('summary');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [attribute, label, count]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved summary CSV: /content/outputs/clause_classification_summary.csv\n",
            "Saved details JSON: /content/outputs/clause_classification_details.json\n",
            "Valid classified clauses:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "repr_error": "Out of range float values are not JSON compliant: nan",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-d45ede6d-f3f0-41af-9d0a-fcfb15a9e9bf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clause_id</th>\n",
              "      <th>attribute</th>\n",
              "      <th>template_used</th>\n",
              "      <th>label</th>\n",
              "      <th>score</th>\n",
              "      <th>rule</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d45ede6d-f3f0-41af-9d0a-fcfb15a9e9bf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d45ede6d-f3f0-41af-9d0a-fcfb15a9e9bf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d45ede6d-f3f0-41af-9d0a-fcfb15a9e9bf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [clause_id, attribute, template_used, label, score, rule, text]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "if not ATTRIBUTE_EXCEL_PATH:\n",
        "    if ON_COLAB:\n",
        "        print(\"Upload the Attribute Dictionary Excel (e.g., 'Attribute Dictionary.xlsx')\")\n",
        "        up = files.upload()\n",
        "        ATTRIBUTE_EXCEL_PATH = list(up.keys())[0]\n",
        "    else:\n",
        "        default = 'data/Attribute Dictionary.xlsx'\n",
        "        if Path(default).exists():\n",
        "            ATTRIBUTE_EXCEL_PATH = default\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Please set ATTRIBUTE_EXCEL_PATH or upload the Excel.\")\n",
        "\n",
        "specs = load_attribute_dictionary(ATTRIBUTE_EXCEL_PATH)\n",
        "\n",
        "if not TEMPLATE_FILES:\n",
        "    if ON_COLAB:\n",
        "        print(\"Upload 1–2 template .txt files (e.g., 'TN_template.txt', 'WA_template.txt')\")\n",
        "        up = files.upload()\n",
        "        TEMPLATE_FILES = list(up.keys())\n",
        "    else:\n",
        "        TEMPLATE_FILES = [str(p) for p in Path('.').glob('*.txt')]\n",
        "        if not TEMPLATE_FILES:\n",
        "            raise FileNotFoundError(\"Please provide template .txt files.\")\n",
        "\n",
        "templates = load_templates(TEMPLATE_FILES)\n",
        "\n",
        "try:\n",
        "    nlp = init_spacy(USE_SPACY_MODEL)\n",
        "except Exception as e:\n",
        "    print(\"[WARN] spaCy init failed:\", e)\n",
        "    nlp = None\n",
        "\n",
        "engines = SimilarityEngines(\n",
        "    sbert_model_name=USE_SBERT_MODEL,\n",
        "    use_cross_encoder=USE_DEBERTA_CROSS_ENCODER,\n",
        "    cross_encoder_name=DEBERTA_CE_MODEL\n",
        ")\n",
        "print(\"SBERT model loaded:\", USE_SBERT_MODEL)\n",
        "print(\"DeBERTa CE enabled:\", USE_DEBERTA_CROSS_ENCODER)\n",
        "\n",
        "if not CONTRACT_FILES:\n",
        "    if ON_COLAB:\n",
        "        print(\"Upload a contract file (PDF preferred; .txt also works).\")\n",
        "        up = files.upload()\n",
        "        CONTRACT_FILES = list(up.keys())\n",
        "    else:\n",
        "        candidates = [str(p) for p in Path('.').glob('*.pdf')] + [str(p) for p in Path('.').glob('*.txt')]\n",
        "        if not candidates:\n",
        "            default_pdf = 'data/Contracts AI Problem Statement - HiLabs Hackathon 2025_IITKGP HiLabs Watermark.pdf'\n",
        "            if Path(default_pdf).exists():\n",
        "                CONTRACT_FILES = [default_pdf]\n",
        "            else:\n",
        "                raise FileNotFoundError(\"Please upload a contract PDF or txt.\")\n",
        "        else:\n",
        "            CONTRACT_FILES = candidates[:1]\n",
        "\n",
        "print(\"Contract files:\", CONTRACT_FILES)\n",
        "\n",
        "all_decisions = []\n",
        "for cpath in CONTRACT_FILES:\n",
        "    if is_pdf(cpath):\n",
        "        raw_text = pdf_to_text(cpath)\n",
        "    else:\n",
        "        raw_text = Path(cpath).read_text(encoding='utf-8', errors='ignore')\n",
        "\n",
        "    clauses = split_into_clauses(raw_text)\n",
        "    print(f\"{Path(cpath).name}: Extracted {len(clauses)} clauses\")\n",
        "\n",
        "    decisions = classify_clauses(clauses, specs, templates, engines)\n",
        "    all_decisions.extend([asdict(d) for d in decisions])\n",
        "\n",
        "df = pd.DataFrame(all_decisions)\n",
        "summary = df[df['label'] != 'Skip'].groupby(['attribute', 'label']).size().reset_index(name='count')\n",
        "print(\"Summary by attribute/label:\")\n",
        "try:\n",
        "    from IPython.display import display \n",
        "    display(summary)\n",
        "except Exception:\n",
        "    print(summary)\n",
        "\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "out_csv = OUT_DIR / \"clause_classification_summary.csv\"\n",
        "out_json = OUT_DIR / \"clause_classification_details.json\"\n",
        "df.to_csv(out_csv, index=False)\n",
        "with open(out_json, 'w', encoding='utf-8') as f:\n",
        "    json.dump(all_decisions, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Saved summary CSV: {out_csv.resolve()}\")\n",
        "print(f\"Saved details JSON: {out_json.resolve()}\")\n",
        "\n",
        "\n",
        "valid_df = df[df['label'].isin(['Standard', 'Non-Standard'])]\n",
        "\n",
        "print(\"Valid classified clauses:\")\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(valid_df[['clause_id', 'attribute', 'template_used', 'label', 'score', 'rule', 'text']])\n",
        "except Exception:\n",
        "    print(valid_df[['clause_id', 'attribute', 'template_used', 'label', 'score', 'rule', 'text']].to_string(index=False))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1749f81",
      "metadata": {
        "id": "a1749f81"
      },
      "outputs": [],
      "source": [
        "\n",
        "import textwrap\n",
        "\n",
        "def pretty_print_clause(decision_row: dict):\n",
        "    print(\"=\"*100)\n",
        "    print(f\"Clause ID: {decision_row['clause_id']} | Attribute: {decision_row.get('attribute')} | Template: {decision_row.get('template_used')}\")\n",
        "    print(f\"Label: {decision_row['label']} | Rule: {decision_row['rule']} | Score: {decision_row['score']:.3f}\")\n",
        "    print(\"- Text:\")\n",
        "    wrapped = textwrap.fill(decision_row['text'], width=100)\n",
        "    print(wrapped)\n",
        "    print(\"- Steps:\")\n",
        "    for st in decision_row['steps']:\n",
        "        sat = \"✅\" if st['satisfied'] else \"❌\"\n",
        "        sc  = \"\" if st['score'] is None else f\" | score={st['score']:.3f}\"\n",
        "        print(f\"  {sat} {st['step']}{sc} — {st['comment']}\")\n",
        "\n",
        "try:\n",
        "    sample = df[df['label'] != 'Skip'].head(5).to_dict(orient='records')\n",
        "    for row in sample:\n",
        "        pretty_print_clause(row)\n",
        "except Exception as e:\n",
        "    print(\"No decisions to display yet.\", e)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
